{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9042aca-73bd-4764-a169-56a8188bd4b9",
   "metadata": {},
   "source": [
    "# GPU Preparation\n",
    "\n",
    "GPU and nvcc (aka cuda) versions should be within the same major version. I've noticed that Ubuntu 22.04 loads on some systems have been way out of **alignment**. Try to get them at the same version.\n",
    "\n",
    "\n",
    "```\n",
    "sudo apt-get purge 'nvidia*' 'cuda*'\n",
    "\n",
    "sudo apt-get install nvidia-driver-535\n",
    "\n",
    "sudo reboot\n",
    "\n",
    "wget https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux.run\n",
    "\n",
    "chmod a+x cuda_12.2.0_535.54.03_linux.run\n",
    " \n",
    "sudo ./cuda_12.2.0_535.54.03_linux.run # And follow the prompts\n",
    "\n",
    "# Edit your .bashrc and put these in. But don't put the hastags in front of them.\n",
    "# export PATH=/usr/local/cuda-12.2/bin${PATH:+:${PATH}}\n",
    "#export LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n",
    "\n",
    "\n",
    "# I also had to do this.  If you cannot type nvcc --version then you need to check the permissions.\n",
    "sudo chmod -R 755 /usr/local/cuda-12.2\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "The results should be something like this:\n",
    "\n",
    "```\n",
    "acshell@ip-10-114-92-249:~$ nvidia-smi | grep -i \"cuda version\" | awk '{print $9}'\n",
    "12.2\n",
    "acshell@ip-10-114-92-249:~$ nvcc --version\n",
    "nvcc: NVIDIA (R) Cuda compiler driver\n",
    "Copyright (c) 2005-2023 NVIDIA Corporation\n",
    "Built on Tue_Jun_13_19:16:58_PDT_2023\n",
    "Cuda compilation tools, release 12.2, V12.2.91\n",
    "Build cuda_12.2.r12.2/compiler.32965470_0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ed9f27e-d2b1-4613-8d06-c06b088ecca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Cuda available? True.\n",
      "Torch Version is 12.1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "print(f\"Is Cuda available? {torch.cuda.is_available()}.\")  # Should return True\n",
    "print(f\"Torch Version is {torch.version.cuda}.\")  # Should return '12.1'\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc4729f-6f5c-4154-9465-ddc6c117b774",
   "metadata": {},
   "source": [
    "### Torch Version\n",
    "\n",
    "PyTorch should be within a minor version of the cuda drivers and cuda drivers need to align with nvidia drivers.  Try hard to make this happen.\n",
    "\n",
    "Here's an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc3da4c-9a5b-433e-a7f4-c8a2c6fdfb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication on CPU took: 6.3072 seconds\n",
      "Result tensor size on CPU: torch.Size([10000, 10000])\n",
      "Matrix multiplication on GPU took: 0.1868 seconds\n",
      "Result tensor size on GPU: torch.Size([10000, 10000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Define the size of the tensors\n",
    "size = 10000\n",
    "\n",
    "# Create two large random tensors for CPU\n",
    "tensor1_cpu = torch.randn(size, size)\n",
    "tensor2_cpu = torch.randn(size, size)\n",
    "\n",
    "# Perform matrix multiplication on the CPU and time it\n",
    "start_time = time.time()\n",
    "result_cpu = torch.matmul(tensor1_cpu, tensor2_cpu)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Matrix multiplication on CPU took: {end_time - start_time:.4f} seconds\")\n",
    "print(f\"Result tensor size on CPU: {result_cpu.size()}\")\n",
    "\n",
    "# Check if CUDA is available and perform the same test on the GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    # Create two large random tensors for GPU\n",
    "    tensor1_gpu = tensor1_cpu.to(device)\n",
    "    tensor2_gpu = tensor2_cpu.to(device)\n",
    "\n",
    "    # Perform matrix multiplication on the GPU and time it\n",
    "    torch.cuda.synchronize()  # Ensure all CUDA operations are finished\n",
    "    start_time = time.time()\n",
    "    result_gpu = torch.matmul(tensor1_gpu, tensor2_gpu)\n",
    "    torch.cuda.synchronize()  # Ensure the GPU has finished the computation\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Matrix multiplication on GPU took: {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"Result tensor size on GPU: {result_gpu.size()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198ff3a5-283a-4f2e-b8db-6e94b79df549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
